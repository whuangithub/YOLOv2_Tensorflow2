{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nimport tensorflow.keras.backend as K\nimport numpy as np\n# image augmentation\nimport imgaug as ia\nfrom imgaug import augmenters as iaa","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\n\nIMAGE_H, IMAGE_W = 512, 512\nGRID_H,  GRID_W = 16, 16 # GRID size = IMAGE size / 32\nNUM_CLASSES = 20\n\nNUM_BOXES = 5 # number of anchors\nANCHORS = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828] # Anchor size\n\nBATCH_SIZE = 10\n\nEPOCHS = 100\n\nLAMBDA_NOOBJECT = 0.5\nLAMBDA_OBJECT = 1\nLAMBDA_CLASS = 1\nLAMBDA_COORD = 5\n\nmax_annot = 0","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"![image.png](images/darknet19.png)","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvBNLeakyRelu(tf.keras.layers.Layer):\n    def __init__(self, num_filters, kernel_size):\n        super().__init__()\n        self.conv = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=kernel_size, padding=\"same\")\n        self.bn = tf.keras.layers.BatchNormalization()\n        # LeakyRelu: see YOLOv1 paper\n        self.leakyrelu = tf.keras.layers.LeakyReLU(alpha=0.1)\n        \n    def call(self, inputs):\n        x = self.conv(inputs)\n        x = self.bn(x)\n        output = self.leakyrelu(x)\n        \n        return output","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DarkNet19Classification(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.convolutional1 = ConvBNLeakyRelu(32, (3, 3))\n        self.maxpool1 = tf.keras.layers.MaxPooling2D(pool_size=[2,2])\n        self.convolutional2 = ConvBNLeakyRelu(64, (3, 3))\n        self.maxpool2 = tf.keras.layers.MaxPooling2D(pool_size=[2,2])\n        self.convolutional3 = ConvBNLeakyRelu(128, (3, 3))\n        self.convolutional4 = ConvBNLeakyRelu(64, (1, 1))\n        self.convolutional5 = ConvBNLeakyRelu(128, (3, 3))\n        self.maxpool3 = tf.keras.layers.MaxPooling2D(pool_size=[2,2])\n        self.convolutional6 = ConvBNLeakyRelu(256, (3, 3))\n        self.convolutional7 = ConvBNLeakyRelu(128, (1, 1))\n        self.convolutional8 = ConvBNLeakyRelu(256, (3, 3))\n        self.maxpool4 = tf.keras.layers.MaxPooling2D(pool_size=[2,2])\n        self.convolutional9 = ConvBNLeakyRelu(512, (3, 3))\n        self.convolutional10 = ConvBNLeakyRelu(256, (1, 1))\n        self.convolutional11 = ConvBNLeakyRelu(512, (3, 3))\n        self.convolutional12 = ConvBNLeakyRelu(256, (1, 1))\n        self.convolutional13 = ConvBNLeakyRelu(512, (3, 3))\n        self.maxpool5 = tf.keras.layers.MaxPooling2D(pool_size=[2,2])\n        self.convolutional14 = ConvBNLeakyRelu(1024, (3, 3))\n        self.convolutional15 = ConvBNLeakyRelu(512, (1, 1))\n        self.convolutional16 = ConvBNLeakyRelu(1024, (3, 3))\n        self.convolutional17 = ConvBNLeakyRelu(512, (1, 1))\n        self.convolutional18 = ConvBNLeakyRelu(1024, (3, 3))\n        # the following is replaced in detection network\n        # 1000: NUM_CLASSES\n        self.convolutional19 = ConvBNLeakyRelu(1000, (1, 1))\n        self.averagepool = tf.keras.layers.GlobalAveragePooling2D()\n        \n    def call(self, inputs):\n        x = self.convolutional1(inputs)\n        x = self.maxpool1(x)\n        x = self.convolutional2(x)\n        x = self.maxpool2(x)\n        x = self.convolutional3(x)\n        x = self.convolutional4(x)\n        x = self.convolutional5(x)\n        x = self.maxpool3(x)\n        x = self.convolutional6(x)\n        x = self.convolutional7(x)\n        x = self.convolutional8(x)\n        x = self.maxpool4(x)\n        x = self.convolutional9(x)\n        x = self.convolutional10(x)\n        x = self.convolutional11(x)\n        x = self.convolutional12(x)\n        x = self.convolutional13(x)\n        x = self.maxpool5(x)\n        x = self.convolutional14(x)\n        x = self.convolutional15(x)\n        x = self.convolutional16(x)\n        x = self.convolutional17(x)\n        x = self.convolutional18(x)\n        x = self.convolutional19(x)\n        x = self.averagepool(x)\n        output = tf.nn.softmax(x)\n        \n        return output","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DarkNet19Detection(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.convolutional1 = ConvBNLeakyRelu(32, (3, 3))\n        self.maxpool1 = tf.keras.layers.MaxPooling2D(pool_size=[2,2])\n        self.convolutional2 = ConvBNLeakyRelu(64, (3, 3))\n        self.maxpool2 = tf.keras.layers.MaxPooling2D(pool_size=[2,2])\n        self.convolutional3 = ConvBNLeakyRelu(128, (3, 3))\n        self.convolutional4 = ConvBNLeakyRelu(64, (1, 1))\n        self.convolutional5 = ConvBNLeakyRelu(128, (3, 3))\n        self.maxpool3 = tf.keras.layers.MaxPooling2D(pool_size=[2,2])\n        self.convolutional6 = ConvBNLeakyRelu(256, (3, 3))\n        self.convolutional7 = ConvBNLeakyRelu(128, (1, 1))\n        self.convolutional8 = ConvBNLeakyRelu(256, (3, 3))\n        self.maxpool4 = tf.keras.layers.MaxPooling2D(pool_size=[2,2])\n        self.convolutional9 = ConvBNLeakyRelu(512, (3, 3))\n        self.convolutional10 = ConvBNLeakyRelu(256, (1, 1))\n        self.convolutional11 = ConvBNLeakyRelu(512, (3, 3))\n        self.convolutional12 = ConvBNLeakyRelu(256, (1, 1))\n        self.convolutional13 = ConvBNLeakyRelu(512, (3, 3))\n        self.maxpool5 = tf.keras.layers.MaxPooling2D(pool_size=[2,2])\n        self.convolutional14 = ConvBNLeakyRelu(1024, (3, 3))\n        self.convolutional15 = ConvBNLeakyRelu(512, (1, 1))\n        self.convolutional16 = ConvBNLeakyRelu(1024, (3, 3))\n        self.convolutional17 = ConvBNLeakyRelu(512, (1, 1))\n        self.convolutional18 = ConvBNLeakyRelu(1024, (3, 3))\n        \n        # the following is replaced\n        # 1000: NUM_CLASSES\n        self.convolutional19 = ConvBNLeakyRelu(1024, (3, 3))\n        self.final_cov = tf.keras.layers.Conv2D(\n            filters=NUM_BOXES*(1+4+NUM_CLASSES),\n            kernel_size=(1,1),\n            padding=\"same\")\n        self.reshape = tf.keras.layers.Reshape([GRID_W, GRID_H, NUM_BOXES, 4 + 1 + NUM_CLASSES])\n\n        self.convolutional_pass_through = ConvBNLeakyRelu(64, (1, 1))\n        self.concat = tf.keras.layers.Concatenate(axis=-1)\n        \n    def call(self, inputs):\n        x = self.convolutional1(inputs)\n        x = self.maxpool1(x)\n        x = self.convolutional2(x)\n        x = self.maxpool2(x)\n        x = self.convolutional3(x)\n        x = self.convolutional4(x)\n        x = self.convolutional5(x)\n        x = self.maxpool3(x)\n        x = self.convolutional6(x)\n        x = self.convolutional7(x)\n        x = self.convolutional8(x)\n        x = self.maxpool4(x)\n        x = self.convolutional9(x)\n        x = self.convolutional10(x)\n        x = self.convolutional11(x)\n        x = self.convolutional12(x)\n        x = self.convolutional13(x)\n        \n        pass_through = x\n        \n        x = self.maxpool5(x)\n        x = self.convolutional14(x)\n        x = self.convolutional15(x)\n        x = self.convolutional16(x)\n        x = self.convolutional17(x)\n        x = self.convolutional18(x)\n        \n        # 224 case: 14*14*512 -> 14*14*64 -> 7*7*256\n        pass_through = self.convolutional_pass_through(pass_through)\n        pass_through = tf.nn.space_to_depth(pass_through, block_size=2)\n        x = self.concat([pass_through, x])\n        \n        x = self.convolutional19(x)\n        x = self.final_cov(x)\n        output = self.reshape(x)\n        \n        return output","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = tfds.load(\"voc\", batch_size=BATCH_SIZE)","execution_count":6,"outputs":[{"output_type":"stream","text":"\u001b[1mDownloading and preparing dataset voc/2007/4.0.0 (download: 868.85 MiB, generated: Unknown size, total: 868.85 MiB) to /root/tensorflow_datasets/voc/2007/4.0.0...\u001b[0m\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1afe78c1a0f446e089d5213cbfceff83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4112adf572714b3b9d6c71e0f77c289e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1105f57011447dda37adef15ea37c5a"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n  InsecureRequestWarning)\n/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n  InsecureRequestWarning)\n","name":"stderr"},{"output_type":"stream","text":"\n\n\n\n\n\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"Shuffling and writing examples to /root/tensorflow_datasets/voc/2007/4.0.0.incompleteVLWYTZ/voc-test.tfrecord\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=4952.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b623ea69b0bd4bb8b8dba74f76ddb811"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"Shuffling and writing examples to /root/tensorflow_datasets/voc/2007/4.0.0.incompleteVLWYTZ/voc-train.tfrecord\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=2501.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4436d1752f994ccea4a18c1b14271c3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"Shuffling and writing examples to /root/tensorflow_datasets/voc/2007/4.0.0.incompleteVLWYTZ/voc-validation.tfrecord\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=2510.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1397aa01648b449f82d36ac188f4ecca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Computing statistics...', max=3.0, style=ProgressStyle(de…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaef8016128b43be94d26188d0b1f81c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"\n\u001b[1mDataset voc downloaded and prepared to /root/tensorflow_datasets/voc/2007/4.0.0. Subsequent calls will reuse this data.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = dataset[\"train\"]\nval_dataset = dataset[\"validation\"]","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"data loading, preprocessing, and augmentation\nReference https://github.com/jmpap/YOLOV2-Tensorflow-2.0\nModified for tensorflow database"},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_generator(input_dataset):\n    '''\n    put bbox and label together\n    '''\n    while True:\n        batch = next(iter(input_dataset))\n    # for batch in input_dataset:\n        # conversion tensor->numpy\n        img = tf.image.resize(batch[\"image\"],(512,512)).numpy()/255.\n        bbox = batch[\"objects\"][\"bbox\"].numpy()\n        label = batch[\"objects\"][\"label\"].numpy()\n        bbox_and_label = []\n        for i in range(img.shape[0]):\n            bbox_and_label.append([])\n            for j in range(len(label[i])):\n                bbox_and_label_item = bbox[i,j].tolist()+[label[i,j]]\n                bbox_and_label[i].append(bbox_and_label_item)\n                \n        # conversion numpy->tensor\n        batch = (tf.convert_to_tensor(img), tf.convert_to_tensor(bbox_and_label))\n        #batch = (img_aug, boxes)\n        yield batch\n        \ndef augmentation_generator(input_dataset):\n    '''\n    Augmented batch generator from a yolo dataset\n\n    Parameters\n    ----------\n    - YOLO dataset\n    \n    Returns\n    -------\n    - augmented batch : tensor (shape : batch_size, IMAGE_W, IMAGE_H, 3)\n        batch : tupple(images, annotations)\n        batch[0] : images : tensor (shape : batch_size, IMAGE_W, IMAGE_H, 3)\n        batch[1] : annotations : tensor (shape : batch_size, max annot, 5)\n    '''\n    while True:\n        batch = next(iter(input_dataset))\n#     for batch in input_dataset:\n        # conversion tensor->numpy\n        img = tf.image.resize(batch[\"image\"],(512,512)).numpy()/255.\n        bbox = batch[\"objects\"][\"bbox\"].numpy()\n        label = batch[\"objects\"][\"label\"].numpy()\n        bbox_and_label = []\n        for i in range(img.shape[0]):\n            bbox_and_label.append([])\n            for j in range(len(label[i])):\n                bbox_and_label_item = bbox[i,j].tolist()+[label[i,j]]\n                bbox_and_label[i].append(bbox_and_label_item)\n        \n        # conversion bbox numpy->ia object\n        ia_boxes = []\n        for i in range(img.shape[0]):\n            ia_bbs = [ia.BoundingBox(x1=bb[0],\n                                       y1=bb[1],\n                                       x2=bb[2],\n                                       y2=bb[3]) for bb in bbox[i]\n                      if (bb[0] + bb[1] +bb[2] + bb[3] > 0)]\n            ia_boxes.append(ia.BoundingBoxesOnImage(ia_bbs, shape=(IMAGE_W, IMAGE_H)))\n        # data augmentation\n        seq = iaa.Sequential([\n            iaa.Fliplr(0.5),\n            iaa.Flipud(0.5),\n            iaa.Multiply((0.4, 1.6)), # change brightness\n            #iaa.ContrastNormalization((0.5, 1.5)),\n            #iaa.Affine(translate_px={\"x\": (-100,100), \"y\": (-100,100)}, scale=(0.7, 1.30))\n            ])\n        #seq = iaa.Sequential([])\n        seq_det = seq.to_deterministic()\n        img_aug = seq_det.augment_images(img)\n        img_aug = np.clip(img_aug, 0, 1)\n        boxes_aug = seq_det.augment_bounding_boxes(ia_boxes)\n        # conversion ia object -> bbox numpy\n        for i in range(img.shape[0]):\n#             boxes_aug[i] = boxes_aug[i].remove_out_of_image().clip_out_of_image()\n            boxes_aug[i] = boxes_aug[i].remove_out_of_image()\n            for j, bb in enumerate(boxes_aug[i].bounding_boxes):\n                bbox_and_label[i][j][0] = bb.x1\n                bbox_and_label[i][j][1] = bb.y1\n                bbox_and_label[i][j][2] = bb.x2\n                bbox_and_label[i][j][3] = bb.y2\n                \n        # conversion numpy->tensor\n        batch = (tf.convert_to_tensor(img_aug), tf.convert_to_tensor(bbox_and_label))\n        #batch = (img_aug, boxes)\n        yield batch\n        \ndef process_true_boxes(true_boxes, anchors, image_width, image_height):\n    '''\n    Build image ground truth in YOLO format from image true_boxes and anchors.\n    \n    Parameters\n    ----------\n    - true_boxes : tensor, shape (max_annot, 5), format : x1 y1 x2 y2 c, coords unit : image pixel\n    - anchors : list [anchor_1_width, anchor_1_height, anchor_2_width, anchor_2_height...]\n        anchors coords unit : grid cell\n    - image_width, image_height : int (pixels)\n    \n    Returns\n    -------\n    - detector_mask : array, shape (GRID_W, GRID_H, anchors_count, 1)\n        1 if bounding box detected by grid cell, else 0\n    - matching_true_boxes : array, shape (GRID_W, GRID_H, anchors_count, 5)\n        Contains adjusted coords of bounding box in YOLO format\n    -true_boxes_grid : array, same shape than true_boxes (max_annot, 5),\n        format : x, y, w, h, c, coords unit : grid cell\n        \n    Note:\n    -----\n    Bounding box in YOLO Format : x, y, w, h, c\n    x, y : center of bounding box, unit : grid cell\n    w, h : width and height of bounding box, unit : grid cell\n    c : label index\n    ''' \n    \n    scale = IMAGE_W / GRID_W # scale = 32\n    \n    anchors_count = len(anchors) // 2\n    anchors = np.array(anchors)\n    anchors = anchors.reshape(len(anchors) // 2, 2)\n    \n    detector_mask = np.zeros((GRID_W, GRID_H, anchors_count, 1))\n    matching_true_boxes = np.zeros((GRID_W, GRID_H, anchors_count, 5))\n    \n    # convert true_boxes numpy array -> tensor\n    true_boxes = true_boxes.numpy()\n    \n    true_boxes_grid = np.zeros(true_boxes.shape)\n    \n    # convert bounding box coords and localize bounding box\n    for i, box in enumerate(true_boxes):\n        # convert box coords to x, y, w, h and convert to grids coord\n        w = (box[2] - box[0]) / scale\n        h = (box[3] - box[1]) / scale    \n        x = ((box[0] + box[2]) / 2) / scale\n        y = ((box[1] + box[3]) / 2) / scale\n        true_boxes_grid[i,...] = np.array([x, y, w, h, box[4]])\n        if w * h > 0: # box exists\n            # calculate iou between box and each anchors and find best anchors\n            best_iou = 0\n            best_anchor = 0\n            for i in range(anchors_count): \n                # iou (anchor and box are shifted to 0,0)\n                intersect = np.minimum(w, anchors[i,0]) * np.minimum(h, anchors[i,1])\n                union = (anchors[i,0] * anchors[i,1]) + (w * h) - intersect\n                iou = intersect / union\n                if iou > best_iou:\n                    best_iou = iou\n                    best_anchor = i\n            # localize box in detector_mask and matching true_boxes\n            if best_iou > 0:\n                x_coord = np.floor(x).astype('int')\n                y_coord = np.floor(y).astype('int')\n                detector_mask[y_coord, x_coord, best_anchor] = 1\n                yolo_box = np.array([x, y, w, h, box[4]])\n                matching_true_boxes[y_coord, x_coord, best_anchor] = yolo_box\n    return matching_true_boxes, detector_mask, true_boxes_grid\n\ndef ground_truth_generator(input_dataset):\n    '''\n    Ground truth batch generator from a yolo dataset, ready to compare with YOLO prediction in loss function.\n\n    Parameters\n    ----------\n    - YOLO dataset. Generate batch:\n        batch : tupple(images, annotations)\n        batch[0] : images : tensor (shape : batch_size, IMAGE_W, IMAGE_H, 3)\n        batch[1] : annotations : tensor (shape : batch_size, max annot, 5)\n        \n    Returns\n    -------\n    - imgs : images to predict. tensor (shape : batch_size, IMAGE_H, IMAGE_W, 3)\n    - detector_mask : tensor, shape (batch_size, GRID_W, GRID_H, anchors_count, 1)\n        1 if bounding box detected by grid cell, else 0\n    - matching_true_boxes : tensor, shape (batch_size, GRID_W, GRID_H, anchors_count, 5)\n        Contains adjusted coords of bounding box in YOLO format\n    - class_one_hot : tensor, shape (batch_size, GRID_W, GRID_H, anchors_count, class_count)\n        One hot representation of bounding box label\n    - true_boxes_grid : annotations : tensor (shape : batch_size, max annot, 5)\n        true_boxes format : x, y, w, h, c, coords unit : grid cell\n    '''\n    while True:\n        batch = next(iter(input_dataset))\n#     for batch in dataset:\n        # imgs\n        imgs = batch[0]\n        \n        # true boxes\n        true_boxes = batch[1]\n        \n        # matching_true_boxes and detector_mask\n        batch_matching_true_boxes = []\n        batch_detector_mask = []\n        batch_true_boxes_grid = []\n        \n        for i in range(true_boxes.shape[0]):     \n            one_matching_true_boxes, one_detector_mask, true_boxes_grid = process_true_boxes(true_boxes[i],\n                                                                                           ANCHORS,\n                                                                                           IMAGE_W,\n                                                                                           IMAGE_H)\n            batch_matching_true_boxes.append(one_matching_true_boxes)\n            batch_detector_mask.append(one_detector_mask)\n            batch_true_boxes_grid.append(true_boxes_grid)\n                \n        detector_mask = tf.convert_to_tensor(np.array(batch_detector_mask), dtype='float32')\n        matching_true_boxes = tf.convert_to_tensor(np.array(batch_matching_true_boxes), dtype='float32')\n        true_boxes_grid = tf.convert_to_tensor(np.array(batch_true_boxes_grid), dtype='float32')\n        \n        # class one_hot\n        matching_classes = tf.keras.backend.cast(matching_true_boxes[..., 4], 'int32') \n        class_one_hot = tf.keras.backend.one_hot(matching_classes, NUM_CLASSES + 1)[:,:,:,:,1:]\n        class_one_hot = tf.cast(class_one_hot, dtype='float32')\n        \n        batch = (imgs, detector_mask, matching_true_boxes, class_one_hot, true_boxes_grid)\n        yield batch","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_train_dataset = augmentation_generator(train_dataset)\ntrain_gen = ground_truth_generator(aug_train_dataset)\nval_gen = ground_truth_generator(val_generator(val_dataset))","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DarkNet19Detection()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def iou(x1, y1, w1, h1, x2, y2, w2, h2):  \n    xmin1 = x1 - 0.5*w1\n    xmax1 = x1 + 0.5*w1\n    ymin1 = y1 - 0.5*h1\n    ymax1 = y1 + 0.5*h1\n    xmin2 = x2 - 0.5*w2\n    xmax2 = x2 + 0.5*w2\n    ymin2 = y2 - 0.5*h2\n    ymax2 = y2 + 0.5*h2\n    interx = np.minimum(xmax1, xmax2) - np.maximum(xmin1, xmin2)\n    intery = np.minimum(ymax1, ymax2) - np.maximum(ymin1, ymin2)\n    inter = interx * intery\n    union = w1*h1 + w2*h2 - inter\n    iou = inter / (union + 1e-6)\n    return iou","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![image.png](images/loss.png)\n(from YOLOv1 paper)","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss\n\ndef yolov2_loss(detector_mask, matching_true_boxes, class_one_hot, true_boxes_grid, y_pred):\n    '''\n    Calculate YOLO V2 loss from prediction (y_pred) and ground truth tensors (detector_mask,\n    matching_true_boxes, class_one_hot, true_boxes_grid,)\n\n    Parameters\n    ----------\n    - detector_mask : tensor, shape (batch, size, GRID_W, GRID_H, anchors_count, 1)\n        1 if bounding box detected by grid cell, else 0\n    - matching_true_boxes : tensor, shape (batch_size, GRID_W, GRID_H, anchors_count, 5)\n        Contains adjusted coords of bounding box in YOLO format\n    - class_one_hot : tensor, shape (batch_size, GRID_W, GRID_H, anchors_count, class_count)\n        One hot representation of bounding box label\n    - true_boxes_grid : annotations : tensor (shape : batch_size, max annot, 5)\n        true_boxes_grid format : x, y, w, h, c (coords unit : grid cell)\n    - y_pred : prediction from model. tensor (shape : batch_size, GRID_W, GRID_H, anchors count, (5 + labels count)\n    \n    Returns\n    -------\n    - loss : scalar\n    - sub_loss : sub loss list : coords loss, class loss and conf loss : scalar\n    '''\n    \n    # anchors tensor\n    anchors = np.array(ANCHORS)\n    anchors = anchors.reshape(len(anchors) // 2, 2)\n    \n    # grid coords tensor\n    coord_x = tf.cast(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)), tf.float32)\n    coord_y = tf.transpose(coord_x, (0,2,1,3,4))\n    coords = tf.tile(tf.concat([coord_x,coord_y], -1), [y_pred.shape[0], 1, 1, 5, 1])\n    \n    # coordinate loss\n    pred_xy = K.sigmoid(y_pred[:,:,:,:,0:2]) # adjust coords between 0 and 1\n    pred_xy = (pred_xy + coords) # add cell coord for comparaison with ground truth. New coords in grid cell unit\n    pred_wh = K.exp(y_pred[:,:,:,:,2:4]) * anchors # adjust width and height for comparaison with ground truth. New coords in grid cell unit\n    #pred_wh = (pred_wh * anchors) # unit : grid cell\n    nb_detector_mask = K.sum(tf.cast(detector_mask > 0.0, tf.float32))\n    xy_loss = LAMBDA_COORD * K.sum(detector_mask * K.square(matching_true_boxes[...,:2] - pred_xy)) / (nb_detector_mask + 1e-6) # Non /2\n    wh_loss = LAMBDA_COORD * K.sum(detector_mask * K.square(K.sqrt(matching_true_boxes[...,2:4]) - \n                                                            K.sqrt(pred_wh))) / (nb_detector_mask + 1e-6)\n    coord_loss = xy_loss + wh_loss\n    \n    # class loss    \n    pred_box_class = y_pred[..., 5:]\n    true_box_class = tf.argmax(class_one_hot, -1)\n    #class_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n    class_loss = K.sparse_categorical_crossentropy(target=true_box_class, output=pred_box_class, from_logits=True)\n    class_loss = K.expand_dims(class_loss, -1) * detector_mask\n    class_loss = LAMBDA_CLASS * K.sum(class_loss) / (nb_detector_mask + 1e-6)\n    \n    # confidence loss\n    pred_conf = K.sigmoid(y_pred[...,4:5])\n    # for each detector : iou between prediction and ground truth\n    x1 = matching_true_boxes[...,0]\n    y1 = matching_true_boxes[...,1]\n    w1 = matching_true_boxes[...,2]\n    h1 = matching_true_boxes[...,3]\n    x2 = pred_xy[...,0]\n    y2 = pred_xy[...,1]\n    w2 = pred_wh[...,0]\n    h2 = pred_wh[...,1]\n    ious = iou(x1, y1, w1, h1, x2, y2, w2, h2)\n    ious = K.expand_dims(ious, -1)\n     \n    # for each detector : best ious between prediction and true_boxes (every bounding box of image)\n    pred_xy = K.expand_dims(pred_xy, 4) # shape : m, GRID_W, GRID_H, BOX, 1, 2 \n    pred_wh = K.expand_dims(pred_wh, 4)\n    pred_wh_half = pred_wh / 2.\n    pred_mins = pred_xy - pred_wh_half\n    pred_maxes = pred_xy + pred_wh_half\n    true_boxe_shape = K.int_shape(true_boxes_grid)\n    true_boxes_grid = K.reshape(true_boxes_grid, [true_boxe_shape[0], 1, 1, 1, true_boxe_shape[1], true_boxe_shape[2]])\n    true_xy = true_boxes_grid[...,0:2]\n    true_wh = true_boxes_grid[...,2:4]\n    true_wh_half = true_wh * 0.5\n    true_mins = true_xy - true_wh_half\n    true_maxes = true_xy + true_wh_half\n    intersect_mins = K.maximum(pred_mins, true_mins) # shape : m, GRID_W, GRID_H, BOX, max_annot, 2 \n    intersect_maxes = K.minimum(pred_maxes, true_maxes) # shape : m, GRID_W, GRID_H, BOX, max_annot, 2\n    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.) # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1] # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n    pred_areas = pred_wh[..., 0] * pred_wh[..., 1] # shape : m, GRID_W, GRID_H, BOX, 1, 1\n    true_areas = true_wh[..., 0] * true_wh[..., 1] # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n    union_areas = pred_areas + true_areas - intersect_areas\n    iou_scores = intersect_areas / union_areas # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n    best_ious = K.max(iou_scores, axis=4)  # Best IOU scores.\n    best_ious = K.expand_dims(best_ious) # shape : m, GRID_W, GRID_H, BOX, 1\n    \n    # no object confidence loss\n    no_object_detection = K.cast(best_ious < 0.6, K.dtype(best_ious)) \n    noobj_mask = no_object_detection * (1 - detector_mask)\n    nb_noobj_mask  = K.sum(tf.cast(noobj_mask  > 0.0, tf.float32))\n    \n    noobject_loss =  LAMBDA_NOOBJECT * K.sum(noobj_mask * K.square(-pred_conf)) / (nb_noobj_mask + 1e-6)\n    # object confidence loss\n    object_loss = LAMBDA_OBJECT * K.sum(detector_mask * K.square(ious - pred_conf)) / (nb_detector_mask + 1e-6)\n    # total confidence loss\n    conf_loss = noobject_loss + object_loss\n    \n    # total loss\n    loss = conf_loss + class_loss + coord_loss\n    sub_loss = [conf_loss, class_loss, coord_loss]  \n              \n    return loss, sub_loss","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gradients\ndef grad(model, img, detector_mask, matching_true_boxes, class_one_hot, true_boxes, training=True):\n    with tf.GradientTape() as tape:\n        y_pred = model(img)\n        loss, sub_loss = yolov2_loss(detector_mask, matching_true_boxes, class_one_hot, true_boxes, y_pred)\n    return loss, sub_loss, tape.gradient(loss, model.trainable_variables)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training\ndef train(epochs, model, train_dataset, val_dataset, steps_per_epoch_train, steps_per_epoch_val, train_name = 'train'):\n    '''\n    Train YOLO model for n epochs.\n    Eval loss on training and validation dataset.\n    Log training loss and validation loss for tensorboard.\n    Save best weights during training (according to validation loss).\n\n    Parameters\n    ----------\n    - epochs : integer, number of epochs to train the model.\n    - model : YOLO model.\n    - train_dataset : YOLO ground truth and image generator from training dataset.\n    - val_dataset : YOLO ground truth and image generator from validation dataset.\n    - steps_per_epoch_train : integer, number of batch to complete one epoch for train_dataset.\n    - steps_per_epoch_val : integer, number of batch to complete one epoch for val_dataset.\n    - train_name : string, training name used to log loss and save weights.\n    \n    Notes :\n    - train_dataset and val_dataset generate YOLO ground truth tensors : detector_mask,\n      matching_true_boxes, class_one_hot, true_boxes_grid. Shape of these tensors (batch size, tensor shape).\n    - steps per epoch = number of images in dataset // batch size of dataset\n    \n    Returns\n    -------\n    - loss history : [train_loss_history, val_loss_history] : list of average loss for each epoch.\n    '''\n    num_epochs = epochs\n    steps_per_epoch_train = steps_per_epoch_train\n    steps_per_epoch_val = steps_per_epoch_val\n    train_loss_history = []\n    val_loss_history = []\n    best_val_loss = 1e6\n    \n    # optimizer\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n    \n    # training\n    for epoch in range(num_epochs):\n        epoch_loss = []\n        epoch_val_loss = []\n        epoch_val_sub_loss = []\n        print('Epoch {} :'.format(epoch))\n        # train\n        for batch_idx in range(steps_per_epoch_train): \n            img, detector_mask, matching_true_boxes, class_one_hot, true_boxes =  next(train_dataset)\n            loss, _, grads = grad(model, img, detector_mask, matching_true_boxes, class_one_hot, true_boxes)\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n            epoch_loss.append(loss)\n            print('-', end='')\n        print(' | ', end='')\n        # val\n        for batch_idx in range(steps_per_epoch_val): \n            img, detector_mask, matching_true_boxes, class_one_hot, true_boxes =  next(val_dataset)\n            loss, sub_loss, grads = grad(model, img, detector_mask, matching_true_boxes, class_one_hot, true_boxes, training=False)\n            epoch_val_loss.append(loss)\n            epoch_val_sub_loss.append(sub_loss)\n            print('-', end='')\n\n        loss_avg = np.mean(np.array(epoch_loss))\n        val_loss_avg = np.mean(np.array(epoch_val_loss))\n        sub_loss_avg = np.mean(np.array(epoch_val_sub_loss), axis=0)\n        train_loss_history.append(loss_avg)\n        val_loss_history.append(val_loss_avg)\n        \n        print(' loss = {:.4f}, val_loss = {:.4f} (conf={:.4f}, class={:.4f}, coords={:.4f})'.format(\n            loss_avg, val_loss_avg, sub_loss_avg[0], sub_loss_avg[1], sub_loss_avg[2]))\n        \n    return [train_loss_history, val_loss_history]","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_train_dataset = augmentation_generator(train_dataset)\ntrain_gen = ground_truth_generator(aug_train_dataset)\nval_gen = ground_truth_generator(val_generator(val_dataset))","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(EPOCHS, model, train_gen, val_gen, 10, 2, 'training_1')","execution_count":19,"outputs":[{"output_type":"stream","text":"Epoch 0 :\n---------- | -- loss = 9.7606, val_loss = 9.9467 (conf=0.3096, class=2.9955, coords=6.6415)\nEpoch 1 :\n---------- | -- loss = 9.7792, val_loss = 9.9419 (conf=0.3094, class=2.9955, coords=6.6371)\nEpoch 2 :\n---------- | -- loss = 9.7678, val_loss = 9.9364 (conf=0.3090, class=2.9955, coords=6.6319)\nEpoch 3 :\n---------- | -- loss = 9.7545, val_loss = 9.9297 (conf=0.3086, class=2.9955, coords=6.6256)\nEpoch 4 :\n---------- | -- loss = 9.7404, val_loss = 9.9214 (conf=0.3082, class=2.9954, coords=6.6178)\nEpoch 5 :\n---------- | -- loss = 9.7451, val_loss = 9.9106 (conf=0.3075, class=2.9954, coords=6.6077)\nEpoch 6 :\n---------- | -- loss = 9.7254, val_loss = 9.8965 (conf=0.3067, class=2.9954, coords=6.5944)\nEpoch 7 :\n---------- | -- loss = 9.7108, val_loss = 9.8779 (conf=0.3055, class=2.9954, coords=6.5770)\nEpoch 8 :\n---------- | -- loss = 9.7073, val_loss = 9.8518 (conf=0.3040, class=2.9954, coords=6.5525)\nEpoch 9 :\n---------- | -- loss = 9.6749, val_loss = 9.8157 (conf=0.3017, class=2.9954, coords=6.5186)\nEpoch 10 :\n---------- | -- loss = 9.6358, val_loss = 9.7671 (conf=0.2986, class=2.9954, coords=6.4731)\nEpoch 11 :\n---------- | -- loss = 9.6058, val_loss = 9.6966 (conf=0.2940, class=2.9954, coords=6.4072)\nEpoch 12 :\n---------- | -- loss = 9.5295, val_loss = 9.5996 (conf=0.2874, class=2.9955, coords=6.3167)\nEpoch 13 :\n---------- | -- loss = 9.4282, val_loss = 9.4640 (conf=0.2775, class=2.9957, coords=6.1908)\nEpoch 14 :\n---------- | -- loss = 9.2937, val_loss = 9.2680 (conf=0.2619, class=2.9960, coords=6.0100)\nEpoch 15 :\n---------- | -- loss = 9.1150, val_loss = 8.9851 (conf=0.2370, class=2.9965, coords=5.7516)\nEpoch 16 :\n---------- | -- loss = 8.8400, val_loss = 8.6029 (conf=0.1986, class=2.9972, coords=5.4071)\nEpoch 17 :\n---------- | -- loss = 8.5024, val_loss = 8.1107 (conf=0.1496, class=2.9978, coords=4.9633)\nEpoch 18 :\n---------- | -- loss = 8.1036, val_loss = 7.6753 (conf=0.2260, class=2.9987, coords=4.4506)\nEpoch 19 :\n---------- | -- loss = 7.8360, val_loss = 7.5825 (conf=0.2176, class=2.9981, coords=4.3667)\nEpoch 20 :\n---------- | -- loss = 7.7105, val_loss = 7.5437 (conf=0.1595, class=2.9973, coords=4.3869)\nEpoch 21 :\n---------- | -- loss = 7.6460, val_loss = 7.4680 (conf=0.1494, class=2.9970, coords=4.3216)\nEpoch 22 :\n---------- | -- loss = 7.4710, val_loss = 7.3346 (conf=0.1631, class=2.9970, coords=4.1745)\nEpoch 23 :\n---------- | -- loss = 7.3987, val_loss = 7.2273 (conf=0.1735, class=2.9969, coords=4.0569)\nEpoch 24 :\n---------- | -- loss = 7.3057, val_loss = 7.1901 (conf=0.1511, class=2.9963, coords=4.0427)\nEpoch 25 :\n---------- | -- loss = 7.1450, val_loss = 7.0408 (conf=0.2179, class=2.9964, coords=3.8266)\nEpoch 26 :\n---------- | -- loss = 7.1349, val_loss = 6.9540 (conf=0.1711, class=2.9957, coords=3.7872)\nEpoch 27 :\n---------- | -- loss = 6.9942, val_loss = 6.9173 (conf=0.1292, class=2.9946, coords=3.7935)\nEpoch 28 :\n---------- | -- loss = 6.8975, val_loss = 6.7543 (conf=0.1364, class=2.9942, coords=3.6237)\nEpoch 29 :\n---------- | -- loss = 6.8088, val_loss = 6.5696 (conf=0.1431, class=2.9939, coords=3.4326)\nEpoch 30 :\n---------- | -- loss = 6.6425, val_loss = 6.3994 (conf=0.1501, class=2.9934, coords=3.2560)\nEpoch 31 :\n---------- | -- loss = 6.4611, val_loss = 6.3059 (conf=0.1235, class=2.9927, coords=3.1897)\nEpoch 32 :\n---------- | -- loss = 6.2208, val_loss = 6.1067 (conf=0.1240, class=2.9926, coords=2.9902)\nEpoch 33 :\n---------- | -- loss = 6.0861, val_loss = 5.8638 (conf=0.1404, class=2.9921, coords=2.7313)\nEpoch 34 :\n---------- | -- loss = 5.8885, val_loss = 5.6966 (conf=0.1162, class=2.9905, coords=2.5899)\nEpoch 35 :\n---------- | -- loss = 5.6205, val_loss = 5.4449 (conf=0.1001, class=2.9898, coords=2.3549)\nEpoch 36 :\n---------- | -- loss = 5.2980, val_loss = 5.1570 (conf=0.0892, class=2.9891, coords=2.0787)\nEpoch 37 :\n---------- | -- loss = 4.9821, val_loss = 4.8133 (conf=0.0900, class=2.9885, coords=1.7347)\nEpoch 38 :\n---------- | -- loss = 4.6052, val_loss = 4.5042 (conf=0.0622, class=2.9881, coords=1.4538)\nEpoch 39 :\n---------- | -- loss = 4.2778, val_loss = 4.1893 (conf=0.0484, class=2.9885, coords=1.1524)\nEpoch 40 :\n---------- | -- loss = 3.9290, val_loss = 3.8274 (conf=0.0493, class=2.9930, coords=0.7851)\nEpoch 41 :\n---------- | -- loss = 3.4895, val_loss = 3.5550 (conf=0.0657, class=3.0002, coords=0.4890)\nEpoch 42 :\n---------- | -- loss = 3.2076, val_loss = 3.4035 (conf=0.0850, class=3.0112, coords=0.3073)\nEpoch 43 :\n---------- | -- loss = 2.9663, val_loss = 3.3199 (conf=0.0945, class=3.0295, coords=0.1960)\nEpoch 44 :\n---------- | -- loss = 3.6158, val_loss = 3.3172 (conf=0.1305, class=3.0718, coords=0.1150)\nEpoch 45 :\n---------- | -- loss = 17.1744, val_loss = 3.3042 (conf=0.1205, class=3.0673, coords=0.1164)\nEpoch 46 :\n---------- | -- loss = 38.4147, val_loss = 3.2863 (conf=0.0879, class=3.0379, coords=0.1606)\nEpoch 47 :\n---------- | -- loss = 7.9529, val_loss = 3.2892 (conf=0.0840, class=3.0375, coords=0.1677)\nEpoch 48 :\n-------","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile\u001b[0;34m(input, multiples, name)\u001b[0m\n\u001b[1;32m  11035\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11036\u001b[0;31m         input, multiples)\n\u001b[0m\u001b[1;32m  11037\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-91dcc63d612d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-17-46fd00bb7aad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, model, train_dataset, val_dataset, steps_per_epoch_train, steps_per_epoch_val, train_name)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_true_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_boxes\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_true_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-fbd592b8fe5a>\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(model, img, detector_mask, matching_true_boxes, class_one_hot, true_boxes, training)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolov2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_true_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-f15cdeeaf706>\u001b[0m in \u001b[0;36myolov2_loss\u001b[0;34m(detector_mask, matching_true_boxes, class_one_hot, true_boxes_grid, y_pred)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# grid coords tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mcoord_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRID_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mGRID_H\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRID_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRID_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mcoord_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoord_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile\u001b[0;34m(input, multiples, name)\u001b[0m\n\u001b[1;32m  11039\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11040\u001b[0m         return tile_eager_fallback(\n\u001b[0;32m> 11041\u001b[0;31m             input, multiples, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m  11042\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11043\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile_eager_fallback\u001b[0;34m(input, multiples, name, ctx)\u001b[0m\n\u001b[1;32m  11079\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tmultiples\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tmultiples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11080\u001b[0m   _result = _execute.execute(b\"Tile\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m> 11081\u001b[0;31m                              ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m  11082\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11083\u001b[0m     _execute.record_gradient(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"The paper trained DarkNet19Classification first.  \nSchedule learning rate"},{"metadata":{},"cell_type":"markdown","source":"# Reference"},{"metadata":{},"cell_type":"markdown","source":"[YOLOv2 Paper](https://arxiv.org/pdf/1612.08242.pdf)  \n[YOLOv1 Paper](https://arxiv.org/pdf/1506.02640.pdf)  \n[jmpap/YOLOV2-Tensorflow-2.0](https://github.com/jmpap/YOLOV2-Tensorflow-2.0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}